{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, pipeline\n",
    "from datasets import ClassLabel, Sequence, load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy import displacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained SciBERT model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\albbl/.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased\\snapshots\\24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\albbl/.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased\\snapshots\\24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\albbl/.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased\\snapshots\\24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\albbl/.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased\\snapshots\\24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\albbl/.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased\\snapshots\\24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\albbl/.cache\\huggingface\\hub\\models--allenai--scibert_scivocab_uncased\\snapshots\\24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and split dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ade_corpus_v2 (C:/Users/albbl/.cache/huggingface/datasets/ade_corpus_v2/Ade_corpus_v2_drug_ade_relation/1.0.0/940d61334dbfac6b01ac5d00286a2122608b8dc79706ee7e9206a1edb172c559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bf3ed844ed4a789eab2d2ebf12a3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_drug_ade_relation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'drug', 'effect', 'indexes'],\n",
       "        num_rows: 6821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Intravenous azithromycin-induced ototoxicity.',\n",
       " 'drug': 'azithromycin',\n",
       " 'effect': 'ototoxicity',\n",
       " 'indexes': {'drug': {'start_char': [12], 'end_char': [24]},\n",
       "  'effect': {'start_char': [33], 'end_char': [44]}}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>drug_indices_start</th>\n",
       "      <th>drug_indices_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intravenous azithromycin-induced ototoxicity.</td>\n",
       "      <td>[azithromycin]</td>\n",
       "      <td>{12}</td>\n",
       "      <td>{24}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immobilization, while Paget's bone disease was...</td>\n",
       "      <td>[dihydrotachysterol]</td>\n",
       "      <td>{91}</td>\n",
       "      <td>{109}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unaccountable severe hypercalcemia in a patien...</td>\n",
       "      <td>[dihydrotachysterol]</td>\n",
       "      <td>{84}</td>\n",
       "      <td>{102}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS: We report two cases of pseudoporphyri...</td>\n",
       "      <td>[naproxen]</td>\n",
       "      <td>{58, 71}</td>\n",
       "      <td>{80, 66}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naproxen, the most common offender, has been a...</td>\n",
       "      <td>[Naproxen]</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{8}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  drug   \n",
       "0      Intravenous azithromycin-induced ototoxicity.        [azithromycin]  \\\n",
       "1  Immobilization, while Paget's bone disease was...  [dihydrotachysterol]   \n",
       "2  Unaccountable severe hypercalcemia in a patien...  [dihydrotachysterol]   \n",
       "3  METHODS: We report two cases of pseudoporphyri...            [naproxen]   \n",
       "4  Naproxen, the most common offender, has been a...            [Naproxen]   \n",
       "\n",
       "  drug_indices_start drug_indices_end  \n",
       "0               {12}             {24}  \n",
       "1               {91}            {109}  \n",
       "2               {84}            {102}  \n",
       "3           {58, 71}         {80, 66}  \n",
       "4                {0}              {8}  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_dataset = {}\n",
    "\n",
    "for row in datasets[\"train\"]:\n",
    "    if row[\"text\"] in consolidated_dataset:\n",
    "        consolidated_dataset[row[\"text\"]][\"drug_indices_start\"].update(row[\"indexes\"][\"drug\"][\"start_char\"])\n",
    "        consolidated_dataset[row[\"text\"]][\"drug_indices_end\"].update(row[\"indexes\"][\"drug\"][\"end_char\"])\n",
    "        \n",
    "    else:\n",
    "        consolidated_dataset[row[\"text\"]] = {\n",
    "            \"text\": row[\"text\"],\n",
    "            \"drug\": [row[\"drug\"]],\n",
    "            # use sets because the indices can repeat for various reasons\n",
    "            \"drug_indices_start\": set(row[\"indexes\"][\"drug\"][\"start_char\"]),\n",
    "            \"drug_indices_end\": set(row[\"indexes\"][\"drug\"][\"end_char\"])\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(list(consolidated_dataset.values()))\n",
    "# for this trial use small subset\n",
    "df = df[:500]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>drug_indices_start</th>\n",
       "      <th>drug_indices_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intravenous azithromycin-induced ototoxicity.</td>\n",
       "      <td>[azithromycin]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immobilization, while Paget's bone disease was...</td>\n",
       "      <td>[dihydrotachysterol]</td>\n",
       "      <td>[91]</td>\n",
       "      <td>[109]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unaccountable severe hypercalcemia in a patien...</td>\n",
       "      <td>[dihydrotachysterol]</td>\n",
       "      <td>[84]</td>\n",
       "      <td>[102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS: We report two cases of pseudoporphyri...</td>\n",
       "      <td>[naproxen]</td>\n",
       "      <td>[58, 71]</td>\n",
       "      <td>[66, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naproxen, the most common offender, has been a...</td>\n",
       "      <td>[Naproxen]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  drug   \n",
       "0      Intravenous azithromycin-induced ototoxicity.        [azithromycin]  \\\n",
       "1  Immobilization, while Paget's bone disease was...  [dihydrotachysterol]   \n",
       "2  Unaccountable severe hypercalcemia in a patien...  [dihydrotachysterol]   \n",
       "3  METHODS: We report two cases of pseudoporphyri...            [naproxen]   \n",
       "4  Naproxen, the most common offender, has been a...            [Naproxen]   \n",
       "\n",
       "  drug_indices_start drug_indices_end  \n",
       "0               [12]             [24]  \n",
       "1               [91]            [109]  \n",
       "2               [84]            [102]  \n",
       "3           [58, 71]         [66, 80]  \n",
       "4                [0]              [8]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"drug_indices_start\"] = df[\"drug_indices_start\"].apply(list).apply(sorted)\n",
    "df[\"drug_indices_end\"] = df[\"drug_indices_end\"].apply(list).apply(sorted)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/albbl/.cache/huggingface/datasets/json/default-eb20db107d0bce16/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b412c48d92c4632bab4b750da4e95a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c1e289758c4cd7bc83f7a0cdedf1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03507885b4344fc88f4f8f08f512408c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/albbl/.cache/huggingface/datasets/json/default-eb20db107d0bce16/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269c32db9bc843ea997201bbb91e5bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'drug', 'drug_indices_start', 'drug_indices_end'],\n",
       "        num_rows: 375\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'drug', 'drug_indices_start', 'drug_indices_end'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save to JSON to then import into Dataset object\n",
    "df.to_json(\"dataset.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "cons_dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\")\n",
    "cons_dataset = cons_dataset[\"train\"].train_test_split()\n",
    "cons_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Labeling\n",
    "\n",
    "O - outside any entity we care about\n",
    "\n",
    "B-DRUG - the beginning of a DRUG entity\n",
    "\n",
    "I-DRUG - inside a DRUG entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-DRUG', 'I-DRUG']\n",
    "\n",
    "custom_seq = Sequence(feature=ClassLabel(num_classes=3, \n",
    "                                         names=label_list,\n",
    "                                         names_file=None, id=None), length=-1, id=None)\n",
    "\n",
    "cons_dataset[\"train\"].features[\"ner_tags\"] = custom_seq\n",
    "cons_dataset[\"test\"].features[\"ner_tags\"] = custom_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_row_labels(row, verbose=False):\n",
    "    \"\"\" Given a row from the consolidated `Ade_corpus_v2_drug_ade_relation` dataset, \n",
    "    generates BIO tags for drug and effect entities. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    text = row[\"text\"]\n",
    "\n",
    "    labels = []\n",
    "    label = \"O\"\n",
    "    prefix = \"\"\n",
    "    \n",
    "    # while iterating through tokens, increment to traverse all drug and effect spans\n",
    "    drug_index = 0\n",
    "    \n",
    "    tokens = tokenizer(text, return_offsets_mapping=True)\n",
    "\n",
    "    for n in range(len(tokens[\"input_ids\"])):\n",
    "        offset_start, offset_end = tokens[\"offset_mapping\"][n]\n",
    "\n",
    "        # should only happen for [CLS] and [SEP]\n",
    "        if offset_end - offset_start == 0:\n",
    "            labels.append(-100)\n",
    "            continue\n",
    "        \n",
    "        if drug_index < len(row[\"drug_indices_start\"]) and offset_start == row[\"drug_indices_start\"][drug_index]:\n",
    "            label = \"DRUG\"\n",
    "            prefix = \"B-\"\n",
    "        \n",
    "        labels.append(label_list.index(f\"{prefix}{label}\"))\n",
    "            \n",
    "        if drug_index < len(row[\"drug_indices_end\"]) and offset_end == row[\"drug_indices_end\"][drug_index]:\n",
    "            label = \"O\"\n",
    "            prefix = \"\"\n",
    "            drug_index += 1\n",
    "\n",
    "        # need to transition \"inside\" if we just entered an entity\n",
    "        if prefix == \"B-\":\n",
    "            prefix = \"I-\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{row}\\n\")\n",
    "        orig = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
    "        for n in range(len(labels)):\n",
    "            print(orig[n], labels[n])\n",
    "    tokens[\"labels\"] = labels\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'OBJECTIVE: The aim of this paper is to describe a case of increased libido during fluvoxamine therapy.', 'drug': ['fluvoxamine'], 'drug_indices_start': [82], 'drug_indices_end': [93]}\n",
      "\n",
      "[CLS] -100\n",
      "objective 0\n",
      ": 0\n",
      "the 0\n",
      "aim 0\n",
      "of 0\n",
      "this 0\n",
      "paper 0\n",
      "is 0\n",
      "to 0\n",
      "describe 0\n",
      "a 0\n",
      "case 0\n",
      "of 0\n",
      "increased 0\n",
      "lib 0\n",
      "##ido 0\n",
      "during 0\n",
      "flu 1\n",
      "##vo 2\n",
      "##xa 2\n",
      "##mine 2\n",
      "therapy 0\n",
      ". 0\n",
      "[SEP] -100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [102, 3201, 862, 111, 2579, 131, 238, 1203, 165, 147, 3401, 106, 820, 131, 1175, 8147, 10612, 781, 1441, 9496, 14301, 22229, 2223, 205, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 9), (9, 10), (11, 14), (15, 18), (19, 21), (22, 26), (27, 32), (33, 35), (36, 38), (39, 47), (48, 49), (50, 54), (55, 57), (58, 67), (68, 71), (71, 74), (75, 81), (82, 85), (85, 87), (87, 89), (89, 93), (94, 101), (101, 102), (0, 0)], 'labels': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, -100]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_row_labels(cons_dataset[\"train\"][2], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0282c555b021457aa6d834cea0bff6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5072bdcc426e4139bc15839018293e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_dataset = cons_dataset.map(generate_row_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=labeled_dataset[\"train\"],\n",
    "    eval_dataset=labeled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: drug, offset_mapping, drug_indices_end, drug_indices_start, text. If drug, offset_mapping, drug_indices_end, drug_indices_start, text are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\albbl\\GitHub\\nlp-project\\.env\\Lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 375\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 120\n",
      "  Number of trainable parameters = 109330179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b596ca67e2407a9f9668215aa776e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9882, 'learning_rate': 9.916666666666668e-06, 'epoch': 0.04}\n",
      "{'loss': 0.8601, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6449, 'learning_rate': 9.75e-06, 'epoch': 0.12}\n",
      "{'loss': 0.5542, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.17}\n",
      "{'loss': 0.5019, 'learning_rate': 9.583333333333335e-06, 'epoch': 0.21}\n",
      "{'loss': 0.4092, 'learning_rate': 9.5e-06, 'epoch': 0.25}\n",
      "{'loss': 0.4348, 'learning_rate': 9.416666666666667e-06, 'epoch': 0.29}\n",
      "{'loss': 0.4177, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.33}\n",
      "{'loss': 0.406, 'learning_rate': 9.250000000000001e-06, 'epoch': 0.38}\n",
      "{'loss': 0.4054, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.42}\n",
      "{'loss': 0.3839, 'learning_rate': 9.083333333333333e-06, 'epoch': 0.46}\n",
      "{'loss': 0.4031, 'learning_rate': 9e-06, 'epoch': 0.5}\n",
      "{'loss': 0.343, 'learning_rate': 8.916666666666667e-06, 'epoch': 0.54}\n",
      "{'loss': 0.2639, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.58}\n",
      "{'loss': 0.2125, 'learning_rate': 8.750000000000001e-06, 'epoch': 0.62}\n",
      "{'loss': 0.2694, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}\n",
      "{'loss': 0.2552, 'learning_rate': 8.583333333333333e-06, 'epoch': 0.71}\n",
      "{'loss': 0.2214, 'learning_rate': 8.5e-06, 'epoch': 0.75}\n",
      "{'loss': 0.1958, 'learning_rate': 8.416666666666667e-06, 'epoch': 0.79}\n",
      "{'loss': 0.2405, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2347, 'learning_rate': 8.25e-06, 'epoch': 0.88}\n",
      "{'loss': 0.1843, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.92}\n",
      "{'loss': 0.1967, 'learning_rate': 8.083333333333334e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: drug, offset_mapping, drug_indices_end, drug_indices_start, text. If drug, offset_mapping, drug_indices_end, drug_indices_start, text are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2328, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae842ae18f1a47588e6688560b111fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12218962609767914, 'eval_precision': 0.7313432835820896, 'eval_recall': 0.7050359712230215, 'eval_f1': 0.717948717948718, 'eval_accuracy': 0.966295609152752, 'eval_runtime': 13.9843, 'eval_samples_per_second': 8.939, 'eval_steps_per_second': 0.572, 'epoch': 1.0}\n",
      "{'loss': 0.1721, 'learning_rate': 7.916666666666667e-06, 'epoch': 1.04}\n",
      "{'loss': 0.1019, 'learning_rate': 7.833333333333333e-06, 'epoch': 1.08}\n",
      "{'loss': 0.1116, 'learning_rate': 7.75e-06, 'epoch': 1.12}\n",
      "{'loss': 0.1194, 'learning_rate': 7.666666666666667e-06, 'epoch': 1.17}\n",
      "{'loss': 0.1484, 'learning_rate': 7.583333333333333e-06, 'epoch': 1.21}\n",
      "{'loss': 0.1403, 'learning_rate': 7.500000000000001e-06, 'epoch': 1.25}\n",
      "{'loss': 0.0915, 'learning_rate': 7.416666666666668e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1428, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}\n",
      "{'loss': 0.108, 'learning_rate': 7.25e-06, 'epoch': 1.38}\n",
      "{'loss': 0.088, 'learning_rate': 7.166666666666667e-06, 'epoch': 1.42}\n",
      "{'loss': 0.1369, 'learning_rate': 7.083333333333335e-06, 'epoch': 1.46}\n",
      "{'loss': 0.0966, 'learning_rate': 7e-06, 'epoch': 1.5}\n",
      "{'loss': 0.0791, 'learning_rate': 6.916666666666667e-06, 'epoch': 1.54}\n",
      "{'loss': 0.0817, 'learning_rate': 6.833333333333334e-06, 'epoch': 1.58}\n",
      "{'loss': 0.1639, 'learning_rate': 6.750000000000001e-06, 'epoch': 1.62}\n",
      "{'loss': 0.0995, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.67}\n",
      "{'loss': 0.1784, 'learning_rate': 6.5833333333333335e-06, 'epoch': 1.71}\n",
      "{'loss': 0.0548, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.75}\n",
      "{'loss': 0.1788, 'learning_rate': 6.416666666666667e-06, 'epoch': 1.79}\n",
      "{'loss': 0.0845, 'learning_rate': 6.333333333333333e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0608, 'learning_rate': 6.25e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0939, 'learning_rate': 6.166666666666667e-06, 'epoch': 1.92}\n",
      "{'loss': 0.1068, 'learning_rate': 6.083333333333333e-06, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: drug, offset_mapping, drug_indices_end, drug_indices_start, text. If drug, offset_mapping, drug_indices_end, drug_indices_start, text are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.027, 'learning_rate': 6e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bcf963562f44b8b6d497c60ca0c885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05824517458677292, 'eval_precision': 0.8428571428571429, 'eval_recall': 0.8489208633093526, 'eval_f1': 0.8458781362007167, 'eval_accuracy': 0.9826839826839827, 'eval_runtime': 13.585, 'eval_samples_per_second': 9.201, 'eval_steps_per_second': 0.589, 'epoch': 2.0}\n",
      "{'loss': 0.0849, 'learning_rate': 5.916666666666667e-06, 'epoch': 2.04}\n",
      "{'loss': 0.0772, 'learning_rate': 5.833333333333334e-06, 'epoch': 2.08}\n",
      "{'loss': 0.0722, 'learning_rate': 5.75e-06, 'epoch': 2.12}\n",
      "{'loss': 0.0815, 'learning_rate': 5.666666666666667e-06, 'epoch': 2.17}\n",
      "{'loss': 0.079, 'learning_rate': 5.583333333333334e-06, 'epoch': 2.21}\n",
      "{'loss': 0.0432, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.25}\n",
      "{'loss': 0.0527, 'learning_rate': 5.416666666666667e-06, 'epoch': 2.29}\n",
      "{'loss': 0.0714, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.33}\n",
      "{'loss': 0.0524, 'learning_rate': 5.2500000000000006e-06, 'epoch': 2.38}\n",
      "{'loss': 0.0513, 'learning_rate': 5.1666666666666675e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0421, 'learning_rate': 5.0833333333333335e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0794, 'learning_rate': 5e-06, 'epoch': 2.5}\n",
      "{'loss': 0.058, 'learning_rate': 4.9166666666666665e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0604, 'learning_rate': 4.833333333333333e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0836, 'learning_rate': 4.75e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0323, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0575, 'learning_rate': 4.583333333333333e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0398, 'learning_rate': 4.5e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0552, 'learning_rate': 4.416666666666667e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0503, 'learning_rate': 4.333333333333334e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0512, 'learning_rate': 4.25e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0455, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0596, 'learning_rate': 4.083333333333334e-06, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: drug, offset_mapping, drug_indices_end, drug_indices_start, text. If drug, offset_mapping, drug_indices_end, drug_indices_start, text are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0307, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebbef6c4b134f7ba8194b921cba347a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.041552767157554626, 'eval_precision': 0.8775510204081632, 'eval_recall': 0.9280575539568345, 'eval_f1': 0.9020979020979022, 'eval_accuracy': 0.987012987012987, 'eval_runtime': 14.4186, 'eval_samples_per_second': 8.669, 'eval_steps_per_second': 0.555, 'epoch': 3.0}\n",
      "{'loss': 0.0432, 'learning_rate': 3.916666666666667e-06, 'epoch': 3.04}\n",
      "{'loss': 0.0623, 'learning_rate': 3.833333333333334e-06, 'epoch': 3.08}\n",
      "{'loss': 0.0235, 'learning_rate': 3.7500000000000005e-06, 'epoch': 3.12}\n",
      "{'loss': 0.0209, 'learning_rate': 3.6666666666666666e-06, 'epoch': 3.17}\n",
      "{'loss': 0.058, 'learning_rate': 3.5833333333333335e-06, 'epoch': 3.21}\n",
      "{'loss': 0.026, 'learning_rate': 3.5e-06, 'epoch': 3.25}\n",
      "{'loss': 0.0472, 'learning_rate': 3.416666666666667e-06, 'epoch': 3.29}\n",
      "{'loss': 0.0405, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}\n",
      "{'loss': 0.0523, 'learning_rate': 3.2500000000000002e-06, 'epoch': 3.38}\n",
      "{'loss': 0.019, 'learning_rate': 3.1666666666666667e-06, 'epoch': 3.42}\n",
      "{'loss': 0.092, 'learning_rate': 3.0833333333333336e-06, 'epoch': 3.46}\n",
      "{'loss': 0.0453, 'learning_rate': 3e-06, 'epoch': 3.5}\n",
      "{'loss': 0.032, 'learning_rate': 2.916666666666667e-06, 'epoch': 3.54}\n",
      "{'loss': 0.0373, 'learning_rate': 2.8333333333333335e-06, 'epoch': 3.58}\n",
      "{'loss': 0.0502, 'learning_rate': 2.7500000000000004e-06, 'epoch': 3.62}\n",
      "{'loss': 0.0198, 'learning_rate': 2.666666666666667e-06, 'epoch': 3.67}\n",
      "{'loss': 0.0572, 'learning_rate': 2.5833333333333337e-06, 'epoch': 3.71}\n",
      "{'loss': 0.0446, 'learning_rate': 2.5e-06, 'epoch': 3.75}\n",
      "{'loss': 0.0518, 'learning_rate': 2.4166666666666667e-06, 'epoch': 3.79}\n",
      "{'loss': 0.0312, 'learning_rate': 2.3333333333333336e-06, 'epoch': 3.83}\n",
      "{'loss': 0.0144, 'learning_rate': 2.25e-06, 'epoch': 3.88}\n",
      "{'loss': 0.0732, 'learning_rate': 2.166666666666667e-06, 'epoch': 3.92}\n",
      "{'loss': 0.0323, 'learning_rate': 2.0833333333333334e-06, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: drug, offset_mapping, drug_indices_end, drug_indices_start, text. If drug, offset_mapping, drug_indices_end, drug_indices_start, text are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0397, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce705d97169437baff5515ed38f7056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.037170976400375366, 'eval_precision': 0.896551724137931, 'eval_recall': 0.935251798561151, 'eval_f1': 0.9154929577464789, 'eval_accuracy': 0.9882498453927026, 'eval_runtime': 12.1115, 'eval_samples_per_second': 10.321, 'eval_steps_per_second': 0.661, 'epoch': 4.0}\n",
      "{'loss': 0.0383, 'learning_rate': 1.916666666666667e-06, 'epoch': 4.04}\n",
      "{'loss': 0.0364, 'learning_rate': 1.8333333333333333e-06, 'epoch': 4.08}\n",
      "{'loss': 0.0143, 'learning_rate': 1.75e-06, 'epoch': 4.12}\n",
      "{'loss': 0.0187, 'learning_rate': 1.6666666666666667e-06, 'epoch': 4.17}\n",
      "{'loss': 0.0333, 'learning_rate': 1.5833333333333333e-06, 'epoch': 4.21}\n",
      "{'loss': 0.0391, 'learning_rate': 1.5e-06, 'epoch': 4.25}\n",
      "{'loss': 0.0465, 'learning_rate': 1.4166666666666667e-06, 'epoch': 4.29}\n",
      "{'loss': 0.0203, 'learning_rate': 1.3333333333333334e-06, 'epoch': 4.33}\n",
      "{'loss': 0.0494, 'learning_rate': 1.25e-06, 'epoch': 4.38}\n",
      "{'loss': 0.02, 'learning_rate': 1.1666666666666668e-06, 'epoch': 4.42}\n",
      "{'loss': 0.0234, 'learning_rate': 1.0833333333333335e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0317, 'learning_rate': 1.0000000000000002e-06, 'epoch': 4.5}\n",
      "{'loss': 0.0341, 'learning_rate': 9.166666666666666e-07, 'epoch': 4.54}\n",
      "{'loss': 0.0194, 'learning_rate': 8.333333333333333e-07, 'epoch': 4.58}\n",
      "{'loss': 0.05, 'learning_rate': 7.5e-07, 'epoch': 4.62}\n",
      "{'loss': 0.0436, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}\n",
      "{'loss': 0.0441, 'learning_rate': 5.833333333333334e-07, 'epoch': 4.71}\n",
      "{'loss': 0.0688, 'learning_rate': 5.000000000000001e-07, 'epoch': 4.75}\n",
      "{'loss': 0.0283, 'learning_rate': 4.1666666666666667e-07, 'epoch': 4.79}\n",
      "{'loss': 0.0156, 'learning_rate': 3.3333333333333335e-07, 'epoch': 4.83}\n",
      "{'loss': 0.0213, 'learning_rate': 2.5000000000000004e-07, 'epoch': 4.88}\n",
      "{'loss': 0.021, 'learning_rate': 1.6666666666666668e-07, 'epoch': 4.92}\n",
      "{'loss': 0.012, 'learning_rate': 8.333333333333334e-08, 'epoch': 4.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: drug, offset_mapping, drug_indices_end, drug_indices_start, text. If drug, offset_mapping, drug_indices_end, drug_indices_start, text are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 125\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0104, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ff38452f194e44a08170cc5e0c9db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.036115240305662155, 'eval_precision': 0.8904109589041096, 'eval_recall': 0.935251798561151, 'eval_f1': 0.9122807017543859, 'eval_accuracy': 0.9891774891774892, 'eval_runtime': 12.1696, 'eval_samples_per_second': 10.272, 'eval_steps_per_second': 0.657, 'epoch': 5.0}\n",
      "{'train_runtime': 804.8272, 'train_samples_per_second': 2.33, 'train_steps_per_second': 0.149, 'train_loss': 0.12576109563621382, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.12576109563621382, metrics={'train_runtime': 804.8272, 'train_samples_per_second': 2.33, 'train_steps_per_second': 0.149, 'train_loss': 0.12576109563621382, 'epoch': 5.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: drug, offset_mapping, drug_indices_end, drug_indices_start, text. If drug, offset_mapping, drug_indices_end, drug_indices_start, text are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 125\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8297c787a054cb4aa8299a7c3c4713f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'DRUG': {'precision': 0.8904109589041096,\n",
       "  'recall': 0.935251798561151,\n",
       "  'f1': 0.9122807017543859,\n",
       "  'number': 139},\n",
       " 'overall_precision': 0.8904109589041096,\n",
       " 'overall_recall': 0.935251798561151,\n",
       " 'overall_f1': 0.9122807017543859,\n",
       " 'overall_accuracy': 0.9891774891774892}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(labeled_dataset[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_ner_model = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_entities(sentence):\n",
    "    tokens = effect_ner_model(sentence)\n",
    "    entities = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        label = int(token[\"entity\"][-1])\n",
    "        if label != 0:\n",
    "            token[\"label\"] = label_list[label]\n",
    "            entities.append(token)\n",
    "    \n",
    "    params = [{\"text\": sentence,\n",
    "               \"ents\": entities,\n",
    "               \"title\": None}]\n",
    "    \n",
    "    html = displacy.render(params, style=\"ent\", manual=True, options={\n",
    "        \"colors\": {\n",
    "                   \"B-DRUG\": \"#f08080\",\n",
    "                   \"I-DRUG\": \"#f08080\",\n",
    "               },\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Abortion, miscarriage or uterine hemorrhage associated with \n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    oprost\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ol\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cyto\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tec\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       "), a labor-inducing drug.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Addiction to many sedatives and analgesics, such as \n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    diaz\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    epa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    m\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       ", morphine, etc.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Birth defects associated with \n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    thal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ido\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mid\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    e\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Bleeding of the intestine associated with \n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    aspirin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-DRUG</span>\n",
       "</mark>\n",
       " therapy</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Cardiovascular disease associated with COX-2 inhibitors (i.e. \n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-DRUG</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    xx\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-DRUG</span>\n",
       "</mark>\n",
       ")</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Deafness and kidney failure associated with \n",
       "<mark class=\"entity\" style=\"background: #f08080; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gentamicin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-DRUG</span>\n",
       "</mark>\n",
       " (an antibiotic)</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Abortion, miscarriage or uterine hemorrhage associated with misoprostol (Cytotec), a labor-inducing drug.\",\n",
    "    \"Addiction to many sedatives and analgesics, such as diazepam, morphine, etc.\",\n",
    "    \"Birth defects associated with thalidomide\",\n",
    "    \"Bleeding of the intestine associated with aspirin therapy\",\n",
    "    \"Cardiovascular disease associated with COX-2 inhibitors (i.e. Vioxx)\",\n",
    "    \"Deafness and kidney failure associated with gentamicin (an antibiotic)\"\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    visualize_entities(example)\n",
    "    print(f\"{'*' * 50}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
